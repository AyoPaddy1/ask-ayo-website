const e=`# Nvidia Q3 FY2025 Earnings: What They Actually Said

**Published:** November 20, 2024  
**Company:** Nvidia Corporation (NVDA)  
**Quarter:** Q3 FY2025 (August - October 2024)

---

## The 30-Second Version

Nvidia keeps breaking records. Revenue hit $35.1 billion, up 94% from last year. Data center revenue alone was $30.8 billion, more than double the same quarter last year. The next-generation Blackwell chips are now in "full production" and demand far exceeds supply. Jensen Huang declared "the age of AI is in full steam." Yet the stock actually dipped slightly after earnings because, well, when you're up 200%+ on the year, expectations are almost impossible to beat. The company guided Q4 to $37.5 billion, above estimates. This is still the most important company in the AI revolution.

---

## The Numbers

| Metric | Actual | Expected | Result |
|--------|--------|----------|--------|
| Revenue | $35.1B | $33.16B | ✅ Beat 6% |
| EPS (adjusted) | $0.81 | $0.75 | ✅ Beat 8% |
| Data Center Revenue | $30.8B | $28.82B | ✅ Beat 7% |
| Q4 Revenue Guidance | $37.5B | $37.08B | ✅ Beat |

Nvidia beat across the board, but the stock fell about 2% in after-hours trading. This is the curse of being the market's favorite: you have to not just beat expectations, but crush them by an enormous margin for the stock to go up. A "mere" 6% revenue beat wasn't enough.

---

## What's Actually Happening

### Data Center Is the Whole Story

Data center revenue was $30.8 billion, up 112% year over year and 17% quarter over quarter. This one segment now represents 88% of Nvidia's total revenue. 

To put the growth in perspective: Nvidia's data center revenue in Q3 alone is more than Intel's entire quarterly revenue. Two years ago, Nvidia's TOTAL annual revenue was $27 billion. Now they're doing more than that in a single quarter just from data center.

The breakdown: About $3.1 billion came from networking (the cables and switches that connect GPUs together). The rest is the chips themselves, primarily the H100/H200 (Hopper architecture) and now ramping Blackwell.

### Blackwell Is "In Full Production"

This was the key update everyone was waiting for. Blackwell is Nvidia's next-generation AI chip architecture, and there had been concerns about production delays and overheating issues.

Jensen Huang put those concerns to rest: Blackwell is in full production, and Nvidia expects to deliver more Blackwell chips in Q4 than previously estimated. He specifically said demand "greatly exceeds supply" and that their supply chain team is working to increase output through next year.

Customer deployments already happening: Microsoft, Oracle, and AWS are already deploying Blackwell-based systems. The chip debuted on MLPerf (the industry benchmark) and delivered 2.2x performance gains over Hopper on large language model workloads.

### Gaming Still Growing (But Not the Focus)

Gaming revenue was $3.3 billion, up 15% from last year. That's solid growth for what used to be Nvidia's core business. But gaming is now just 9% of revenue. The company noted that Q4 gaming revenue will decline sequentially due to supply constraints. Translation: they're prioritizing AI chips over gaming chips in their manufacturing.

### Gross Margins: The One "Concern"

Gross margin was 75%, which is exceptional by any normal standard. But it was down slightly from prior quarters, and CFO Colette Kress warned that margins will dip to the "low 70s" as Blackwell ramps.

This isn't really concerning though. Early production always has lower margins as you work out manufacturing kinks. The shift to more complex Blackwell systems (which include multiple chips, networking, and cooling) also means more components and complexity. Management expects margins to climb back to mid-70s as production scales.

---

## The Big Question

**Is AI demand sustainable or is this a bubble?**

Jensen Huang addressed this directly on the call: "There's been a lot of talk about an AI bubble. From our vantage point, we see something very different."

His argument: We're just at the beginning of three simultaneous scaling trends:

1. **Pre-training scaling:** Making models bigger and training them on more data. This is the original AI scaling law and it's "intact and continuing."

2. **Post-training scaling:** Using techniques like reinforcement learning and synthetic data to improve models after initial training. This is newer and creating additional demand.

3. **Inference scaling:** Running AI models at scale for real applications (ChatGPT, Copilot, etc.). This is exploding as more companies deploy AI products.

The bulls see infinite demand ahead. The bears worry that at some point, the $30-50 billion being spent annually on Nvidia chips needs to generate actual business returns for customers. So far, the hyperscalers (Microsoft, Google, Amazon, Meta) keep increasing their AI spending, which suggests they're seeing returns.

---

## What Management Said

> "The age of AI is in full steam, propelling a global shift to Nvidia computing."
> — Jensen Huang, CEO

Jensen isn't being subtle. Everyone is buying Nvidia chips. Everyone. And he's betting they'll keep buying for years. When the CEO of a $3 trillion company says something this bullish, he's either right or setting himself up for a spectacular fall.

> "Demand for Hopper and anticipation for Blackwell — in full production — are incredible as foundation model makers scale pretraining, post-training and inference."
> — Jensen Huang

Current chips are selling out. Next-gen chips are selling out before they can make them. That's the position every company dreams of. The question is how long it lasts.

> "We have created a whole new industry called AI factories."
> — Jensen Huang

This is Jensen reframing the narrative. Data centers used to just store and process data. Now they're factories that produce intelligence. And Nvidia makes the machines for those factories. It's a clever way to explain why spending on GPUs should keep growing.

> "Our leadership in inference is extraordinary. GB200, NVLink 72, is 10 times, 10-15 times higher performance... Our leadership there is surely multi-year."
> — Jensen Huang, on Blackwell's inference capabilities

Even if AMD or others catch up on training chips, Nvidia believes they're so far ahead on inference (actually running AI applications) that competitors will take years to close the gap. This matters because inference is where the long-term volume is.

---

## Jargon, Explained

**Hopper vs Blackwell:** These are the names of Nvidia's GPU architectures, named after famous scientists (Grace Hopper and David Blackwell). Hopper (H100, H200) is the current generation that powered the AI boom. Blackwell (B100, B200, GB200) is the next generation, roughly 2-2.5x faster for AI workloads.

**NVLink 72:** A high-speed connection that links up to 72 GPUs together so they can work as one massive system. This matters because the largest AI models are too big to fit on a single chip. The ability to connect more chips together faster is a key advantage.

**Inference vs Training:** Training is teaching an AI model (very compute intensive, done once). Inference is using the trained model (less compute per request, but done billions of times). Think of training as creating a recipe, inference as cooking from that recipe.

**Agentic AI:** AI that doesn't just answer questions but takes actions. Like an AI assistant that can actually book your travel, file your expenses, or write and deploy code. This requires even more compute because the AI needs to reason through multi-step tasks.

**Sovereign AI:** Countries building their own AI infrastructure rather than relying on US cloud providers. Nvidia sees this as a multi-billion dollar opportunity as nations like Japan, India, and EU countries invest in domestic AI capabilities.

---

## What's Next?

**Q4 FY2025 Guidance:**
- Revenue: $37.5 billion (+/- 2%)
- Gross margin: 73.0-73.5% (dipping as Blackwell ramps)
- Data center: Continued strength in Hopper, initial Blackwell ramp
- Gaming: Expected to decline sequentially (supply constraints)

The implied sequential growth of 7% would be the "slowest" in several quarters, but we're talking about adding another $2-3 billion in a single quarter. Nvidia is now so large that even "slow" growth is massive in dollar terms.

**Key Things to Watch:**
- Blackwell production ramp and any yield issues
- China export restrictions (Nvidia doesn't include China data center revenue in guidance)
- Whether hyperscaler AI capex continues at current levels
- AMD MI300 competitive positioning

**Key Dates:**
- February 2025: Q4 FY2025 earnings
- March 2025: GTC conference (expect major announcements)

---

## The Bottom Line

Nvidia is in a class of its own. Revenue up 94% year over year. Data center up 112%. Full production on next-gen chips with demand exceeding supply. Multiple years of technology leadership ahead.

The "risk" is that the stock has priced in so much success that even crushing estimates doesn't move shares much. At roughly $3.5 trillion market cap, Nvidia needs to keep delivering 50%+ growth just to justify current valuations.

But here's the thing: they keep doing exactly that. Every quarter, analysts say expectations are too high, and every quarter Nvidia beats them. Until that changes, betting against this company has been a losing trade.

The real question isn't whether Nvidia will grow, but whether the rest of the economy can absorb enough AI to justify the massive investments being made. For now, the hyperscalers keep spending, and Nvidia keeps winning.

---

*This summary is for educational purposes and is not financial advice. Data sourced from Nvidia's Q3 FY2025 earnings release and conference call.*
`;export{e as default};
